{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The need for RNNs arise from the fact that fuzzy neural network (FNNs) are unable to capture time based dependencies in data.\n",
    "\n",
    "RNNs are used frequently for and have tremendous results in tasks such as NLP, Machine Translation and Algorithmic Trading.\n",
    "\n",
    "In RNN, each output is dependent on the previous one, which allows them capture dependencies in sequences. (e.g. in language, next word depends on previous word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model sequences, we need to:\n",
    " 1. Handle variable-length sequences\n",
    " 2. Track long-term dependencies\n",
    " 3. Maintain information about order\n",
    " 4. Share parameters across the sequence\n",
    "\n",
    "Today: Recurrent Neural Networks (RNNs) as an approach to sequence modeling problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is used for sequential data problems. It iterates through the sequence elements and maintains a state containing information relative to what it has been seen so far.\n",
    "\n",
    "In effect, an RNN is a type of neural network that has an internal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for RNN: <br/>\n",
    "1. $x_{t}$ is the input at time step t. For example, $x_{1}$ could be a one-hot vector corresponding to the second word of a sentence.\n",
    "2. $s_{t}$ is the hidden state at time step t. It’s the “memory” of the network.  $s_{t}$ is calculated based on the previous hidden state and the input at the current step:  $s_{t}$=f(U $x_{t}$ + W $s_{t-1}$). The function f usually is a nonlinearity such as tanh or ReLU.   $s_{-1}$, which is required to calculate the first hidden state, is typically initialized to all zeroes.\n",
    "3. $o_{t}$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. <br/>\n",
    "$o_{t}$ = softmax(V$s_{t}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being dependent on the previous computations.\n",
    "\n",
    "Unfold or unroll mean writing up network for full sequence.\n",
    "\n",
    "For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U, W, V remains same at each step, because of this RNN is more memory intensive and need to trained for monger in comparison to RNN.\n",
    "\n",
    "Reason for same weights at each step: Separate paramters are unable to generalize to sequence length that aren't encountered during training process.\n",
    "\n",
    "Intuitively, we can think f this as having a sequence ($x_{1}$, $x_{2}$, ....., $x_{t}$) where we are trying to find the P($x_{t+1}$|$x_{t}$, $x_{t-1}$, $x_{t-2}$, ...., $x_{1}$)\n",
    "\n",
    "\n",
    "RNN Model Topology:\n",
    "1. One-to-One (one input, one output)\n",
    "2. One-to-Many (one input, many outputs)\n",
    "3. Many-to-One (many inputs, one output)\n",
    "4. Many-to-Many (multiple inputs and outputs where no of inputs = no of outputs)\n",
    "5. Many-to-Many (multiple inputs and outputs where no of inputs != no of outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for RNN:\n",
    "input_sequence = [] # vector representation of input\n",
    "s_t = 0\n",
    "for x_t in input_sequence:\n",
    "    o_t = activation(dot(U, x_t) + dot(W, s_t) + b)\n",
    "    s_t = o_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Implementation of a Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 100 # number of timesteps in the input sequence\n",
    "input_features = 32 # dimensionality of input feature space\n",
    "output_features = 64 # dimensionality of output feature space\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((output_features, ))\n",
    "\n",
    "U = np.random.random((output_features, input_features))\n",
    "W = np.random.random((output_features, output_features))\n",
    "V = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "success_outputs = []\n",
    "\n",
    "for x_t in inputs:\n",
    "    state_t = np.tanh(np.dot(U, x_t) + np.dot(W, state_t) + b)\n",
    "    o_t = softmax(np.dot(V, state_t))\n",
    "    success_outputs.append(o_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_sequence = np.concatenate(success_outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44227454e-03, 1.08536523e-03, 4.43459078e-05, 5.37241939e-02,\n",
       "       8.54148853e-02, 7.83333548e-04, 1.44987756e-02, 3.24604339e-04,\n",
       "       5.73214573e-03, 5.65427507e-03, 1.04581250e-02, 3.41979746e-05,\n",
       "       1.76262880e-03, 1.11982628e-04, 1.59859904e-05, 1.08534095e-05,\n",
       "       8.23672608e-05, 1.60411720e-03, 1.36498807e-02, 2.46639595e-02,\n",
       "       2.46634069e-04, 5.16599592e-03, 3.42905761e-03, 1.43004569e-03,\n",
       "       9.78768236e-06, 3.72034768e-04, 1.69641486e-02, 4.86755356e-03,\n",
       "       8.41974386e-05, 5.60756902e-05, 7.82076218e-03, 6.64289437e-03,\n",
       "       7.18332788e-03, 1.77527097e-03, 1.63904767e-01, 1.56633307e-03,\n",
       "       7.30257035e-04, 6.67034392e-03, 3.02139586e-04, 1.02356066e-03,\n",
       "       3.86993331e-01, 3.10809787e-04, 9.70005012e-04, 2.59502030e-02,\n",
       "       4.30630990e-04, 6.06291098e-06, 8.33450206e-03, 1.34531544e-03,\n",
       "       4.24672398e-03, 3.76258316e-04, 1.04474604e-02, 1.15546668e-04,\n",
       "       5.39744547e-03, 1.35795488e-03, 1.15677659e-03, 7.19408596e-02,\n",
       "       1.97732283e-03, 5.35164586e-03, 1.28520640e-04, 3.49645219e-04,\n",
       "       1.40425736e-02, 6.96591874e-03, 2.35362110e-04, 2.25638964e-04,\n",
       "       1.44227281e-03, 1.08536005e-03, 4.43458958e-05, 5.37243211e-02,\n",
       "       8.54150197e-02, 7.83335601e-04, 1.44987544e-02, 3.24605794e-04,\n",
       "       5.73214592e-03, 5.65428813e-03, 1.04581251e-02, 3.41979981e-05,\n",
       "       1.76262300e-03, 1.11982977e-04, 1.59859214e-05, 1.08533440e-05,\n",
       "       8.23669390e-05, 1.60413256e-03, 1.36498399e-02, 2.46639574e-02,\n",
       "       2.46634220e-04, 5.16595955e-03, 3.42907256e-03, 1.43004544e-03,\n",
       "       9.78764193e-06, 3.72033732e-04, 1.69641386e-02, 4.86756443e-03,\n",
       "       8.41971097e-05, 5.60756994e-05, 7.82074286e-03, 6.64289716e-03,\n",
       "       7.18336711e-03, 1.77527593e-03, 1.63903865e-01, 1.56633222e-03])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output_sequence[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36442242, 0.80936581, 0.91212408, ..., 0.7241877 , 0.04158229,\n",
       "        0.26876221],\n",
       "       [0.23420591, 0.27569583, 0.40458744, ..., 0.26082021, 0.33589759,\n",
       "        0.55422131],\n",
       "       [0.57100382, 0.89665303, 0.82936618, ..., 0.12513132, 0.48662522,\n",
       "        0.99469631],\n",
       "       ...,\n",
       "       [0.51047852, 0.97326324, 0.89371166, ..., 0.88828527, 0.4564248 ,\n",
       "        0.01728221],\n",
       "       [0.58678564, 0.95309764, 0.25554656, ..., 0.51015233, 0.1723988 ,\n",
       "        0.6965287 ],\n",
       "       [0.732482  , 0.59642324, 0.47269101, ..., 0.7254335 , 0.48181679,\n",
       "        0.98339069]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
